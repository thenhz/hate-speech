{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model loaded...\n"
     ]
    }
   ],
   "source": [
    "data_root = '/home/thenhz/workspace/hate-speech/data/'\n",
    "\n",
    "GLOVE_MODEL_FILE = data_root + 'word2vec/glove_WIKI'\n",
    "\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec.load(GLOVE_MODEL_FILE)\n",
    "print(\"word2vec model loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/site-packages/ipykernel_launcher.py:78: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('italian')\n",
    "\n",
    "data = pd.read_csv('data/hate-speech/haspeede_TW-train.tsv', sep='\\t', names=['text','sentiment'])\n",
    "print(\"data file loaded..\")\n",
    "\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "#preprocess\n",
    "\n",
    "def preprocess_tweet(text, stopwords, FLAGS = FLAGS):\n",
    "# Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "    \n",
    "    def hashtag(text):\n",
    "        text = text.group()\n",
    "        hashtag_body = text[1:]\n",
    "        return hashtag_body\n",
    "    \n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\")\n",
    "    text = re_sub(r\"/\",\"\")\n",
    "    text = re_sub(r\"@\\w+\", \"\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<NHZSMILE>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<NHZLOLFACE>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<NHZSADFACE>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<NHZNEUTRALFACE>\")\n",
    "    text = re_sub(r\"<3\",\"<NHZ_HEART>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NHZNUMBERS>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", \"\")\n",
    "    #text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <NHZ_ELONG>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    #text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "    \n",
    "    #text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    #remove punctuation\n",
    "    #text_no_punct = ''\n",
    "    #for c in text:\n",
    "    #    if c not in punctuation:\n",
    "    #        text_no_punct += c\n",
    "    #    else:\n",
    "    #        text_no_punct += ' '\n",
    "    #text = text_no_punct\n",
    "    \n",
    "    words = re.findall(r'[a-z]+', text.lower())\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    \n",
    "    #remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_stop_words(train_sentences, stopwords):\n",
    "    count = 0\n",
    "    for i, sentence in train_sentences.iterrows():\n",
    "        #print(sentence['text'])\n",
    "        #if(count>10):\n",
    "        #    break\n",
    "        #count+=1\n",
    "        new_sent = preprocess_tweet(sentence['text'],stopwords)\n",
    "        #print(new_sent)\n",
    "        #new_sent = ' '.join([word for word in new_sent.split() if word not in stop_words])\n",
    "        #print(new_sent)\n",
    "        train_sentences.set_value(i, 'text', new_sent, takeable=False)\n",
    "    return train_sentences\n",
    "\n",
    "train_sentences = filter_stop_words(data, nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>576268275560116224</th>\n",
       "      <td>invasione animali forse offesa animali stranie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815833978691260416</th>\n",
       "      <td>terrorismo mettere stato soggezione persone re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815112299044372480</th>\n",
       "      <td>infatti finch guadagnato campi rom ok alemanno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577451440047853568</th>\n",
       "      <td>piovegovernolad italia stranieri culo va bene ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844921803575382016</th>\n",
       "      <td>londonattack chiedete buonisti cavolo cosa vog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811099342593462272</th>\n",
       "      <td>corriere tangenti mafia capitale dimenticatama...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833323806873833473</th>\n",
       "      <td>perch quando migranti israeliti arrivarono ter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835493194796314624</th>\n",
       "      <td>divertimento giorno trovare patrioti italiani ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839605324684361730</th>\n",
       "      <td>modena comune paga benzina nomadi portano figl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851906322283016192</th>\n",
       "      <td>altro islam cristianesimo dobbiamo sorbire ign...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834785125258911744</th>\n",
       "      <td>grazie stef giusto caricando messo salvini ava...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782928453872062464</th>\n",
       "      <td>mons liberati nhznumbers anni europa sar islam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821056358283902977</th>\n",
       "      <td>altro profughi zavorre uomini</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783280663365574656</th>\n",
       "      <td>piene palleil giorno ricordo morte migrantie u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818135548489515009</th>\n",
       "      <td>smettetela dire italiani stati migranti tratta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826013678726160385</th>\n",
       "      <td>responsabilit crisi economica valori sociali m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835894111928528896</th>\n",
       "      <td>cattiveria diritto vita migranti giro finita l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848835878969540612</th>\n",
       "      <td>minorenne rom arrestato dopo furto appartamento</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810120475074555908</th>\n",
       "      <td>studentessa cinese morta fermato nomade nhznum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578609578423808000</th>\n",
       "      <td>stefano ora italia cominci difendersi islamism...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835948142122127360</th>\n",
       "      <td>ddl calderoli velo islamico apologia sharia di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824571157269639168</th>\n",
       "      <td>roma rapina campo rom barbuta tre arresti ombr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826378364827099136</th>\n",
       "      <td>rischiando essere presa pazza sa cosa parlatru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792613642533539840</th>\n",
       "      <td>proposito immancabile iphone mano finti profug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801383022608191488</th>\n",
       "      <td>povert esclusione scarsa istruzione nhznumbers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826202983004246016</th>\n",
       "      <td>roma torino grazie collegamento campi rom quin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854427382039683072</th>\n",
       "      <td>floris presentatore senza spina dorsale dovres...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822158606632124417</th>\n",
       "      <td>zucchero te amico negro oppure rom visto fate ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799650731779358720</th>\n",
       "      <td>profughi ancora malvedenti malpensanti maldice...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848953272941637636</th>\n",
       "      <td>sar caso stata colpita citt occidentale russia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827614048992784384</th>\n",
       "      <td>niger risultato accordi anti migranti aumentat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784623293445922816</th>\n",
       "      <td>arrigoni centro migranti cremeno quasi clandes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816217760187486209</th>\n",
       "      <td>furia violenta islamici arriva foggia giovane ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821115711175225345</th>\n",
       "      <td>quintacolonna islamici peste genere umano matr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551988544361988096</th>\n",
       "      <td>complimenti islamici x aver relegato donne sot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786329124273057800</th>\n",
       "      <td>essere musulmani roma andare pregare</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823657132151214080</th>\n",
       "      <td>presadiretta auguro torture fatte egittopaese ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563464340851130370</th>\n",
       "      <td>bella testata plutonio tocca tocca terroristi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789137844967137280</th>\n",
       "      <td>prefetto arezzo visita castelfranco piandisc i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825408756376858624</th>\n",
       "      <td>italia ridicola fa parlare rom devastano sporc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831536055820095488</th>\n",
       "      <td>finti mussulmani vittime uhhhhhhhh finti terro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840850479093600256</th>\n",
       "      <td>emma bonino convention lingotto senza figli im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824998697935175682</th>\n",
       "      <td>roma arrestati cinque ladri rom acrobati mentr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840228788755652608</th>\n",
       "      <td>solo turisti stranieri amano italia nhznumbers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845159172169617408</th>\n",
       "      <td>roma scatta allerta terrorismo pi paura black ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830437603442884610</th>\n",
       "      <td>italia piena clandestini potenziali delinquent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815170234202148865</th>\n",
       "      <td>stretta terrorismo maggiori controlli allontan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590261842469072899</th>\n",
       "      <td>giornalisti sciacalli tutta italia odiando cuo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821719461409517568</th>\n",
       "      <td>facebook ebreo zuckerberg cuculo sradicato sra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790984955396489217</th>\n",
       "      <td>europa renzi batte pugni migranti poi parlano ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782881912247447553</th>\n",
       "      <td>giovent italiana sud nord quando immigrati toc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808003813646991360</th>\n",
       "      <td>italia multirazziale succede migranti rom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829801083102978048</th>\n",
       "      <td>nato costruisce baluardo antiterrorismo italia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596031227146174464</th>\n",
       "      <td>perch profughi mandriano casa quel ministro di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805717998099120128</th>\n",
       "      <td>india dopo uae sviluppa qatar cooperazione cyb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846367935056826369</th>\n",
       "      <td>crescere cupa perillo campo rom dentro scampia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826099380746870789</th>\n",
       "      <td>prima manda ufficio immigrazione albergo ora r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810986816463601664</th>\n",
       "      <td>immigrati pagano pensione bisogna accoglierne ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813337116428091392</th>\n",
       "      <td>intervista choc imam roma gi musulmana islam m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817747285836791809</th>\n",
       "      <td>gabrielli ospite befana sap ansa sottovalutiam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "576268275560116224  invasione animali forse offesa animali stranie...   \n",
       "815833978691260416  terrorismo mettere stato soggezione persone re...   \n",
       "815112299044372480  infatti finch guadagnato campi rom ok alemanno...   \n",
       "577451440047853568  piovegovernolad italia stranieri culo va bene ...   \n",
       "844921803575382016  londonattack chiedete buonisti cavolo cosa vog...   \n",
       "811099342593462272  corriere tangenti mafia capitale dimenticatama...   \n",
       "833323806873833473  perch quando migranti israeliti arrivarono ter...   \n",
       "835493194796314624  divertimento giorno trovare patrioti italiani ...   \n",
       "839605324684361730  modena comune paga benzina nomadi portano figl...   \n",
       "851906322283016192  altro islam cristianesimo dobbiamo sorbire ign...   \n",
       "834785125258911744  grazie stef giusto caricando messo salvini ava...   \n",
       "782928453872062464  mons liberati nhznumbers anni europa sar islam...   \n",
       "821056358283902977                      altro profughi zavorre uomini   \n",
       "783280663365574656  piene palleil giorno ricordo morte migrantie u...   \n",
       "818135548489515009  smettetela dire italiani stati migranti tratta...   \n",
       "826013678726160385  responsabilit crisi economica valori sociali m...   \n",
       "835894111928528896  cattiveria diritto vita migranti giro finita l...   \n",
       "848835878969540612    minorenne rom arrestato dopo furto appartamento   \n",
       "810120475074555908  studentessa cinese morta fermato nomade nhznum...   \n",
       "578609578423808000  stefano ora italia cominci difendersi islamism...   \n",
       "835948142122127360  ddl calderoli velo islamico apologia sharia di...   \n",
       "824571157269639168  roma rapina campo rom barbuta tre arresti ombr...   \n",
       "826378364827099136  rischiando essere presa pazza sa cosa parlatru...   \n",
       "792613642533539840  proposito immancabile iphone mano finti profug...   \n",
       "801383022608191488  povert esclusione scarsa istruzione nhznumbers...   \n",
       "826202983004246016  roma torino grazie collegamento campi rom quin...   \n",
       "854427382039683072  floris presentatore senza spina dorsale dovres...   \n",
       "822158606632124417  zucchero te amico negro oppure rom visto fate ...   \n",
       "799650731779358720  profughi ancora malvedenti malpensanti maldice...   \n",
       "848953272941637636  sar caso stata colpita citt occidentale russia...   \n",
       "...                                                               ...   \n",
       "827614048992784384  niger risultato accordi anti migranti aumentat...   \n",
       "784623293445922816  arrigoni centro migranti cremeno quasi clandes...   \n",
       "816217760187486209  furia violenta islamici arriva foggia giovane ...   \n",
       "821115711175225345  quintacolonna islamici peste genere umano matr...   \n",
       "551988544361988096  complimenti islamici x aver relegato donne sot...   \n",
       "786329124273057800               essere musulmani roma andare pregare   \n",
       "823657132151214080  presadiretta auguro torture fatte egittopaese ...   \n",
       "563464340851130370  bella testata plutonio tocca tocca terroristi ...   \n",
       "789137844967137280  prefetto arezzo visita castelfranco piandisc i...   \n",
       "825408756376858624  italia ridicola fa parlare rom devastano sporc...   \n",
       "831536055820095488  finti mussulmani vittime uhhhhhhhh finti terro...   \n",
       "840850479093600256  emma bonino convention lingotto senza figli im...   \n",
       "824998697935175682  roma arrestati cinque ladri rom acrobati mentr...   \n",
       "840228788755652608  solo turisti stranieri amano italia nhznumbers...   \n",
       "845159172169617408  roma scatta allerta terrorismo pi paura black ...   \n",
       "830437603442884610  italia piena clandestini potenziali delinquent...   \n",
       "815170234202148865  stretta terrorismo maggiori controlli allontan...   \n",
       "590261842469072899  giornalisti sciacalli tutta italia odiando cuo...   \n",
       "821719461409517568  facebook ebreo zuckerberg cuculo sradicato sra...   \n",
       "790984955396489217  europa renzi batte pugni migranti poi parlano ...   \n",
       "782881912247447553  giovent italiana sud nord quando immigrati toc...   \n",
       "808003813646991360          italia multirazziale succede migranti rom   \n",
       "829801083102978048     nato costruisce baluardo antiterrorismo italia   \n",
       "596031227146174464  perch profughi mandriano casa quel ministro di...   \n",
       "805717998099120128  india dopo uae sviluppa qatar cooperazione cyb...   \n",
       "846367935056826369  crescere cupa perillo campo rom dentro scampia...   \n",
       "826099380746870789  prima manda ufficio immigrazione albergo ora r...   \n",
       "810986816463601664  immigrati pagano pensione bisogna accoglierne ...   \n",
       "813337116428091392  intervista choc imam roma gi musulmana islam m...   \n",
       "817747285836791809  gabrielli ospite befana sap ansa sottovalutiam...   \n",
       "\n",
       "                    sentiment  \n",
       "576268275560116224          1  \n",
       "815833978691260416          0  \n",
       "815112299044372480          0  \n",
       "577451440047853568          1  \n",
       "844921803575382016          1  \n",
       "811099342593462272          0  \n",
       "833323806873833473          0  \n",
       "835493194796314624          0  \n",
       "839605324684361730          0  \n",
       "851906322283016192          0  \n",
       "834785125258911744          0  \n",
       "782928453872062464          1  \n",
       "821056358283902977          1  \n",
       "783280663365574656          1  \n",
       "818135548489515009          0  \n",
       "826013678726160385          1  \n",
       "835894111928528896          1  \n",
       "848835878969540612          0  \n",
       "810120475074555908          0  \n",
       "578609578423808000          1  \n",
       "835948142122127360          0  \n",
       "824571157269639168          0  \n",
       "826378364827099136          0  \n",
       "792613642533539840          1  \n",
       "801383022608191488          0  \n",
       "826202983004246016          0  \n",
       "854427382039683072          1  \n",
       "822158606632124417          0  \n",
       "799650731779358720          0  \n",
       "848953272941637636          0  \n",
       "...                       ...  \n",
       "827614048992784384          0  \n",
       "784623293445922816          0  \n",
       "816217760187486209          1  \n",
       "821115711175225345          1  \n",
       "551988544361988096          1  \n",
       "786329124273057800          0  \n",
       "823657132151214080          1  \n",
       "563464340851130370          1  \n",
       "789137844967137280          0  \n",
       "825408756376858624          0  \n",
       "831536055820095488          1  \n",
       "840850479093600256          0  \n",
       "824998697935175682          0  \n",
       "840228788755652608          0  \n",
       "845159172169617408          0  \n",
       "830437603442884610          1  \n",
       "815170234202148865          0  \n",
       "590261842469072899          1  \n",
       "821719461409517568          1  \n",
       "790984955396489217          0  \n",
       "782881912247447553          1  \n",
       "808003813646991360          0  \n",
       "829801083102978048          0  \n",
       "596031227146174464          1  \n",
       "805717998099120128          0  \n",
       "846367935056826369          0  \n",
       "826099380746870789          0  \n",
       "810986816463601664          0  \n",
       "813337116428091392          0  \n",
       "817747285836791809          0  \n",
       "\n",
       "[2998 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4052\n",
      "1944\n"
     ]
    }
   ],
   "source": [
    "#data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['sentiment'] == 0].size) #positive\n",
    "print(data[ data['sentiment'] == 1].size) #negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8353"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_model.wv\n",
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730613"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIM = 100\n",
    "\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(max_fatures, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/thenhz/anaconda3/envs/nlp36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 21, 100)           200000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 21, 196)           232848    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 21, 32)            29312     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 8)                 1312      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 463,490\n",
      "Trainable params: 463,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, WV_DIM,weights=[wv_matrix], input_length = X.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out,return_sequences=True, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(8))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(32, return_sequences=True,\n",
    "#               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "#model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "#model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='rmsprop',\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2008, 21) (2008, 2)\n",
      "(990, 21) (990, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1807 samples, validate on 201 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.1358 - acc: 0.9552 - val_loss: 0.9848 - val_acc: 0.7015\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.1162 - acc: 0.9629 - val_loss: 1.1432 - val_acc: 0.6965\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.1079 - acc: 0.9646 - val_loss: 1.1318 - val_acc: 0.6965\n",
      "Epoch 4/5\n",
      " - 3s - loss: 0.0930 - acc: 0.9707 - val_loss: 1.1718 - val_acc: 0.6915\n",
      "Epoch 5/5\n",
      " - 4s - loss: 0.0881 - acc: 0.9745 - val_loss: 1.2393 - val_acc: 0.6567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4bc1d45ba8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 5, validation_split = 0.1, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score = model.evaluate(X_test, Y_test, batch_size = batch_size)\n",
    "#logger.info('mse=%f, mae=%f, mape=%f' % (scores[0],scores[1],scores[2]))\n",
    "#print(score)\n",
    "#print(\"score: %.2f \" % (score))\n",
    "#print(\"acc: %.2f\" % (acc))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 59.86842105263158 %\n",
      "neg_acc 78.134110787172 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
